{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "limiting-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import mindspore.communication.management as D\n",
    "from mindspore.communication.management import get_rank\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.nn.wrap.loss_scale import DynamicLossScaleUpdateCell\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, TimeMonitor\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "#from mindspore.train.train_thor import ConvertModelUtils\n",
    "from mindspore.nn.optim import Lamb, Momentum, AdamWeightDecay#, THOR\n",
    "from mindspore import log as logger\n",
    "from mindspore.common import set_seed\n",
    "from src import BertNetworkWithLoss, BertTrainOneStepCell, BertTrainOneStepWithLossScaleCell, \\\n",
    "                BertTrainAccumulationAllReduceEachWithLossScaleCell, \\\n",
    "                BertTrainAccumulationAllReducePostWithLossScaleCell, \\\n",
    "                BertTrainOneStepWithLossScaleCellForAdam, \\\n",
    "                AdamWeightDecayForBert\n",
    "from src.dataset import create_bert_dataset\n",
    "from src.config import cfg, bert_net_cfg\n",
    "from src.utils import LossCallBack, BertLearningRate\n",
    "from mindspore.nn.metrics import Metric\n",
    "import mindspore.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chronic-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2308:140291543226176,MainProcess):2021-02-07-15:31:33.350.011 [mindspore/train/serialization.py:386] 81 parameters in the net are not loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bert.bert.bert_encoder.layers.0.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.0.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.0.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.0.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.0.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.0.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.1.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.1.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.1.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.1.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.1.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.1.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.2.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.2.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.2.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.2.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.2.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.2.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.3.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.3.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.3.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.3.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.3.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.3.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.4.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.4.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.4.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.4.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.4.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.4.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.5.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.5.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.5.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.5.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.5.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.5.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.6.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.6.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.6.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.6.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.6.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.6.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.7.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.7.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.7.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.7.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.7.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.7.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.8.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.8.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.8.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.8.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.8.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.8.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.9.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.9.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.9.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.9.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.9.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.9.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.10.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.10.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.10.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.10.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.10.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.10.attention.attention.value_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.11.attention.attention.query_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.11.attention.attention.query_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.11.attention.attention.key_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.11.attention.attention.key_layer.bias',\n",
       " 'bert.bert.bert_encoder.layers.11.attention.attention.value_layer.weight',\n",
       " 'bert.bert.bert_encoder.layers.11.attention.attention.value_layer.bias',\n",
       " 'bert.bert.dense.weight',\n",
       " 'bert.bert.dense.bias',\n",
       " 'bert.cls1.output_bias',\n",
       " 'bert.cls1.dense.weight',\n",
       " 'bert.cls1.dense.bias',\n",
       " 'bert.cls1.layernorm.gamma',\n",
       " 'bert.cls1.layernorm.beta',\n",
       " 'bert.cls2.dense.weight',\n",
       " 'bert.cls2.dense.bias']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_with_loss = BertNetworkWithLoss(bert_net_cfg, True)\n",
    "param_dict = load_checkpoint('2.ckpt')\n",
    "load_param_into_net(net_with_loss, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "historic-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myMetric(Metric):\n",
    "    '''\n",
    "    Self-defined Metric as a callback.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(myMetric, self).__init__()\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.total_num = 0\n",
    "        self.acc_num = 0\n",
    "\n",
    "    def update(self, *inputs):\n",
    "        total_num = self._convert_data(inputs[0])\n",
    "        acc_num = self._convert_data(inputs[1])\n",
    "        self.total_num = total_num\n",
    "        self.acc_num = acc_num\n",
    "\n",
    "    def eval(self):\n",
    "        return self.acc_num/self.total_num\n",
    "\n",
    "\n",
    "class GetLogProbs(nn.Cell):\n",
    "    '''\n",
    "    Get MaskedLM prediction scores\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(GetLogProbs, self).__init__()\n",
    "        self.bert = BertModel(config, False)\n",
    "        self.cls1 = GetMaskedLMOutput(config)\n",
    "\n",
    "    def construct(self, input_ids, input_mask, token_type_id, masked_pos):\n",
    "        sequence_output, _, embedding_table = self.bert(input_ids, token_type_id, input_mask)\n",
    "        prediction_scores = self.cls1(sequence_output, embedding_table, masked_pos)\n",
    "        return prediction_scores\n",
    "\n",
    "\n",
    "class BertPretrainEva(nn.Cell):\n",
    "    '''\n",
    "    Evaluate MaskedLM prediction scores\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(BertPretrainEva, self).__init__()\n",
    "        self.bert = GetLogProbs(config)\n",
    "        self.argmax = P.Argmax(axis=-1, output_type=mstype.int32)\n",
    "        self.equal = P.Equal()\n",
    "        self.mean = P.ReduceMean()\n",
    "        self.sum = P.ReduceSum()\n",
    "        self.total = Parameter(Tensor([0], mstype.float32))\n",
    "        self.acc = Parameter(Tensor([0], mstype.float32))\n",
    "        self.reshape = P.Reshape()\n",
    "        self.shape = P.Shape()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "\n",
    "    def construct(self, input_ids, input_mask, token_type_id, masked_pos, masked_ids, masked_weights, nsp_label):\n",
    "        \"\"\"Calculate prediction scores\"\"\"\n",
    "        bs, _ = self.shape(input_ids)\n",
    "        probs = self.bert(input_ids, input_mask, token_type_id, masked_pos)\n",
    "        index = self.argmax(probs)\n",
    "        index = self.reshape(index, (bs, -1))\n",
    "        eval_acc = self.equal(index, masked_ids)\n",
    "        eval_acc1 = self.cast(eval_acc, mstype.float32)\n",
    "        real_acc = eval_acc1 * masked_weights\n",
    "        acc = self.sum(real_acc)\n",
    "        total = self.sum(masked_weights)\n",
    "        self.total += total\n",
    "        self.acc += acc\n",
    "        return acc, self.total, self.acc\n",
    "\n",
    "\n",
    "def get_enwiki_512_dataset(batch_size=1, repeat_count=1, distribute_file=''):\n",
    "    '''\n",
    "    Get enwiki dataset when seq_length is 512.\n",
    "    '''\n",
    "    ds = de.TFRecordDataset([cfg.data_file], cfg.schema_file, columns_list=[\"input_ids\", \"input_mask\", \"segment_ids\",\n",
    "                                                                            \"masked_lm_positions\", \"masked_lm_ids\",\n",
    "                                                                            \"masked_lm_weights\",\n",
    "                                                                            \"next_sentence_labels\"])\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "    ds = ds.map(operations=type_cast_op, input_columns=\"segment_ids\")\n",
    "    ds = ds.map(operations=type_cast_op, input_columns=\"input_mask\")\n",
    "    ds = ds.map(operations=type_cast_op, input_columns=\"input_ids\")\n",
    "    ds = ds.map(operations=type_cast_op, input_columns=\"masked_lm_ids\")\n",
    "    ds = ds.map(operations=type_cast_op, input_columns=\"masked_lm_positions\")\n",
    "    ds = ds.map(operations=type_cast_op, input_columns=\"next_sentence_labels\")\n",
    "    ds = ds.repeat(repeat_count)\n",
    "\n",
    "    # apply batch operations\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def bert_predict():\n",
    "    '''\n",
    "    Predict function\n",
    "    '''\n",
    "    devid = int(0)\n",
    "    context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\", device_id=devid)\n",
    "    net_for_pretraining = BertPretrainEva(bert_net_cfg)\n",
    "    net_for_pretraining.set_train(False)\n",
    "    param_dict = load_checkpoint(\"2.ckpt\")\n",
    "    load_param_into_net(net_for_pretraining, param_dict)\n",
    "    model = Model(net_for_pretraining)\n",
    "    return model, net_for_pretraining\n",
    "\n",
    "\n",
    "def MLM_eval():\n",
    "    '''\n",
    "    Evaluate function\n",
    "    '''\n",
    "    _,  net_for_pretraining = bert_predict()\n",
    "    net = Model(net_for_pretraining, eval_network=net_for_pretraining, eval_indexes=[0, 1, 2],\n",
    "                metrics={'name': myMetric()})\n",
    "    res = net.eval(dataset, dataset_sink_mode=False)\n",
    "    print(\"==============================================================\")\n",
    "    for _, v in res.items():\n",
    "        print(\"Accuracy is: \")\n",
    "        print(v)\n",
    "    print(\"==============================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "arranged-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset.text as text\n",
    "\n",
    "input_tx = \"[CLS] [MASK] [MASK] [MASK] 是中国神魔小说的经典之作，与《三国演义》《水浒传》《红楼梦》并称为中国古典四大名著。[SEP]\"\n",
    "\n",
    "vof=open('vocab.txt',encoding='utf-8')\n",
    "vocab = text.Vocab.from_list([i for i in vof.readline().strip()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "funny-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_op = text.BertTokenizer(vocab=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bearing-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "dataset = ds.TextFileDataset('input.txt', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ranging-dealing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [MASK] [MASK] [MASK] 是中国神魔小说的经典之作，与《三国演义》《水浒传》《红楼梦》并称为中国古典四大名著。[SEP]\n"
     ]
    }
   ],
   "source": [
    "for data in dataset.create_dict_iterator(output_numpy=True):\n",
    "    print(text.to_str(data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "referenced-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------after tokenization-----------------------------\n",
      "['[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]'\n",
      " '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]'\n",
      " '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]'\n",
      " '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]'\n",
      " '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]' '[UNK]'\n",
      " '[UNK]' '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(operations=tokenizer_op)\n",
    "\n",
    "print(\"------------------------after tokenization-----------------------------\")\n",
    "\n",
    "for i in dataset.create_dict_iterator(num_epochs=1, output_numpy=True):\n",
    "    print(text.to_str(i['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-concord",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
